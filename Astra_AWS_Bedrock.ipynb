{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amarjit420/AWS_Bedrock/blob/main/Astra_AWS_Bedrock.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6715bc2b",
      "metadata": {
        "id": "6715bc2b"
      },
      "source": [
        "# Vector Similarity Astra-Bedrock Search QA Quickstart\n",
        "\n",
        "Set up a simple Question-Answering system with LangChain and Amazon Bedrock, using Astra DB as the Vector Database."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "761d9b70",
      "metadata": {
        "id": "761d9b70"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "Make sure you have a vector-capable Astra database (get one for free at [astra.datastax.com](https://astra.datastax.com)):\n",
        "\n",
        "- You will be asked to provide the **Database ID** for your Astra DB instance (see [here](https://awesome-astra.github.io/docs/pages/astra/faq/#where-should-i-find-a-database-identifier) for details);\n",
        "- Ensure you have an **Access Token** for your database with role _Database Administrator_ (see [here](https://awesome-astra.github.io/docs/pages/astra/create-token/) for details).\n",
        "\n",
        "Likewise, you will need the credentials to your Amazon Web Services identity, with access to **Amazon Bedrock**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18548e31",
      "metadata": {
        "id": "18548e31"
      },
      "source": [
        "## Set up your Python environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "042f832e",
      "metadata": {
        "tags": [],
        "id": "042f832e"
      },
      "outputs": [],
      "source": [
        "%pip install --quiet \\\n",
        "  \"cassio>=0.1.3\" \\\n",
        "  \"langchain==0.0.249\" \\\n",
        "  \"boto3==1.28.62\" \\\n",
        "  \"botocore==1.31.62\" \\\n",
        "  \"cohere==4.37\" \\\n",
        "  \"openai==1.3.7\" \\\n",
        "  \"tiktoken==0.5.2\" \\\n",
        "  \"awscli==1.29.62\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c840842",
      "metadata": {
        "id": "6c840842"
      },
      "source": [
        "## Import needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b243d1b4",
      "metadata": {
        "tags": [],
        "id": "b243d1b4"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import sys\n",
        "from getpass import getpass\n",
        "\n",
        "\n",
        "import boto3\n",
        "import cassio\n",
        "\n",
        "from langchain.embeddings import BedrockEmbeddings\n",
        "from langchain.llms import Bedrock\n",
        "from langchain.vectorstores import Cassandra\n",
        "from langchain.schema import Document\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.document_loaders import TextLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fccce0c2",
      "metadata": {
        "id": "fccce0c2"
      },
      "source": [
        "## Astra DB Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a9b0a92b-f9ce-4810-a8ef-5741b2449b18",
      "metadata": {
        "tags": [],
        "id": "a9b0a92b-f9ce-4810-a8ef-5741b2449b18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cc6a389-b61f-4684-8444-dfb381943f15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Astra DB ID ('0123abcd-'):a5e92896-5d1c-4d1f-81f9-4866bda8e022\n",
            "Enter your Astra DB Token ('AstraCS:...'):··········\n",
            "Enter your keyspace name (optional, default keyspace used if not provided):\n"
          ]
        }
      ],
      "source": [
        "ASTRA_DB_ID = input(\"Enter your Astra DB ID ('0123abcd-'):\")\n",
        "ASTRA_DB_APPLICATION_TOKEN = getpass(\"Enter your Astra DB Token ('AstraCS:...'):\")\n",
        "ASTRA_DB_KEYSPACE = input(\"Enter your keyspace name (optional, default keyspace used if not provided):\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a066f378-8fdb-4d4b-a7b1-bf685fbfd413",
      "metadata": {
        "tags": [],
        "id": "a066f378-8fdb-4d4b-a7b1-bf685fbfd413",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a894e318-ff2f-498b-ba52-40486aa48e76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for a5e92896-5d1c-4d1f-81f9-4866bda8e022-us-east1.db.astra.datastax.com:29042:3a2bb189-103b-4499-9dc2-3c3edd0645d6. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for a5e92896-5d1c-4d1f-81f9-4866bda8e022-us-east1.db.astra.datastax.com:29042:3a2bb189-103b-4499-9dc2-3c3edd0645d6. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "ERROR:cassandra.connection:Closing connection <AsyncoreConnection(133678386692928) a5e92896-5d1c-4d1f-81f9-4866bda8e022-us-east1.db.astra.datastax.com:29042:3a2bb189-103b-4499-9dc2-3c3edd0645d6> due to protocol error: Error from server: code=000a [Protocol error] message=\"Beta version of the protocol used (5/v5-beta), but USE_BETA flag is unset\"\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 5 to 4 for a5e92896-5d1c-4d1f-81f9-4866bda8e022-us-east1.db.astra.datastax.com:29042:3a2bb189-103b-4499-9dc2-3c3edd0645d6. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n"
          ]
        }
      ],
      "source": [
        "cassio.init(\n",
        "    token=ASTRA_DB_APPLICATION_TOKEN,\n",
        "    database_id=ASTRA_DB_ID,\n",
        "    keyspace=ASTRA_DB_KEYSPACE if ASTRA_DB_KEYSPACE else None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33c674f1",
      "metadata": {
        "id": "33c674f1"
      },
      "source": [
        "## AWS Credentials Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3219cb02-f955-4fb3-85af-3f149868958a",
      "metadata": {
        "id": "3219cb02-f955-4fb3-85af-3f149868958a"
      },
      "source": [
        "_Note_: in the following cells you will be asked to explicitly provide the credentials to your AWS account. These are set as environment variables for usage by the subsequent `boto3` calls. Please refer to [boto3's documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html) on the possible ways to supply your credentials in a more production-like environment.\n",
        "\n",
        "In particular, if you are running this notebook in **Amazon SageMaker Studio**, please note that it is sufficient to add the Bedrock policy to your SageMaker role, as outlined at [this link](https://github.com/aws-samples/amazon-bedrock-workshop#enable-aws-iam-permissions-for-bedrock), to access the Bedrock services. In that case you can skip the following three setup cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ccbb76ea",
      "metadata": {
        "tags": [],
        "id": "ccbb76ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40f1a68a-3f65-42bd-d269-9e1936c60d83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your AWS Access Key ID:··········\n"
          ]
        }
      ],
      "source": [
        "# Input your AWS Access Key ID\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = getpass(\"Your AWS Access Key ID:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f93c873d",
      "metadata": {
        "tags": [],
        "id": "f93c873d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eec86291-6a19-4e14-d1bf-9f49db2530e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your AWS Secret Access Key:··········\n"
          ]
        }
      ],
      "source": [
        "# Input your AWS Secret Access Key\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = getpass(\"Your AWS Secret Access Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "737249f5",
      "metadata": {
        "tags": [],
        "id": "737249f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69e0a3d4-46d1-4d00-803b-d134c5786736"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your AWS Session Token:··········\n"
          ]
        }
      ],
      "source": [
        "# Input your AWS Session Token\n",
        "os.environ[\"AWS_SESSION_TOKEN\"] = getpass(\"Your AWS Session Token:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#AWS Titan models\n",
        "\n",
        "Amazon Titan Foundation Models are pre-trained on large datasets, making them powerful, general-purpose models. Use them as is, or customize them by fine tuning the models with your own data for a particular task without annotating large volumes of data.\n",
        "\n",
        "\n",
        "There are three types of Titan models, embeddings, text generation, and image generation.\n",
        "\n",
        "\n",
        "There are two Titan Embeddings models. The Titan Embeddings G1 – Text model translate text inputs (words, phrases or possibly large units of text) into numerical representations (known as embeddings) that contain the semantic meaning of the text. While this LLM will not generate text, it is useful for applications like personalization and search. By comparing embeddings, the model will produce more relevant and contextual responses than word matching. The new Titan Multimodal Embeddings G1 model is used for use cases like searching image by text, by image for similarity or by a combination of text and image. It translates the input image or text into an embedding that contain the semantic meaning of both the image and text in the same semantic space.\n",
        "\n",
        "\n",
        "Titan Text models are generative LLMs for tasks such as summarization, text generation (for example, creating a blog post), classification, open-ended Q&A, and information extraction. They are also trained on many different programming languages as well as rich text format like tables, JSON and csv’s among others.\n",
        "\n",
        "\n",
        "Titan Image Generator G1 is a generative foundation model that generates images from natural language text. This model can also be used to edit or generate variations for an existing or a generated image.\n",
        "\n"
      ],
      "metadata": {
        "id": "WpSofn5hh0lV"
      },
      "id": "WpSofn5hh0lV"
    },
    {
      "cell_type": "markdown",
      "id": "4388ac1d",
      "metadata": {
        "id": "4388ac1d"
      },
      "source": [
        "## Set up AWS Bedrock objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d65c46f0",
      "metadata": {
        "tags": [],
        "id": "d65c46f0"
      },
      "outputs": [],
      "source": [
        "bedrock_runtime = boto3.client(\"bedrock-runtime\", \"us-west-2\")\n",
        "bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\",\n",
        "                                       client=bedrock_runtime)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29d9f48b",
      "metadata": {
        "id": "29d9f48b"
      },
      "source": [
        "## Set up the Vector Store\n",
        "\n",
        "This command will create a suitable table in your database if it does not exist yet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "29d9f48c",
      "metadata": {
        "tags": [],
        "id": "29d9f48c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "outputId": "c1ee3433-ef4b-4b55-abcf-b09b2aba1a5d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/embeddings/bedrock.py\u001b[0m in \u001b[0;36m_embedding_func\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             response = self.client.invoke_model(\n\u001b[0m\u001b[1;32m    121\u001b[0m                 \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    979\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mClientError\u001b[0m: An error occurred (UnrecognizedClientException) when calling the InvokeModel operation: The security token included in the request is invalid.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-5905b01a40b8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m vector_store = Cassandra(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0membedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbedrock_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtable_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"shakespeare_act5\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# <-- meaning: use the global defaults from cassio.init()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mkeyspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# <-- meaning: use the global defaults from cassio.init()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/vectorstores/cassandra.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embedding, session, keyspace, table_name, ttl_seconds)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mkeyspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeyspace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtable_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0membedding_dimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_embedding_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mprimary_key_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"TEXT\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/vectorstores/cassandra.py\u001b[0m in \u001b[0;36m_get_embedding_dimension\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedding_dimension\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             self._embedding_dimension = len(\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This is a sample sentence.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             )\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedding_dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/embeddings/bedrock.py\u001b[0m in \u001b[0;36membed_query\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mEmbeddings\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \"\"\"\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedding_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/embeddings/bedrock.py\u001b[0m in \u001b[0;36m_embedding_func\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse_body\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"embedding\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error raised by inference endpoint: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     def embed_documents(\n",
            "\u001b[0;31mValueError\u001b[0m: Error raised by inference endpoint: An error occurred (UnrecognizedClientException) when calling the InvokeModel operation: The security token included in the request is invalid."
          ]
        }
      ],
      "source": [
        "vector_store = Cassandra(\n",
        "    embedding=bedrock_embeddings,\n",
        "    table_name=\"shakespeare_act5\",\n",
        "    session=None,  # <-- meaning: use the global defaults from cassio.init()\n",
        "    keyspace=None,  # <-- meaning: use the global defaults from cassio.init()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6af24199",
      "metadata": {
        "id": "6af24199"
      },
      "source": [
        "## Populate the database\n",
        "\n",
        "Add lines for the text of \"Romeo and Astra\", Scene 5, Act 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1dab5114",
      "metadata": {
        "tags": [],
        "id": "1dab5114",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f431f15-7555-4c33-bc51-b0409ebfc6db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 75985  100 75985    0     0   181k      0 --:--:-- --:--:-- --:--:--  181k\n"
          ]
        }
      ],
      "source": [
        "# retrieve the text of a scene from act 5 of Romeo and Astra.\n",
        "# Juliet's name was changed to Astra to prevent the LLM from \"cheating\" when providing an answer.\n",
        "! mkdir -p \"texts\"\n",
        "! curl \"https://raw.githubusercontent.com/awesome-astra/docs/main/docs/pages/aiml/aws/bedrock_resources/romeo_astra.json\" \\\n",
        "    --output \"texts/romeo_astra.json\"\n",
        "input_lines = json.load(open(\"texts/romeo_astra.json\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cb6b732",
      "metadata": {
        "id": "4cb6b732"
      },
      "source": [
        "Next, you'll populate the database with the lines from the play.\n",
        "This can take a couple of minutes, please be patient.  In total there are 321 lines.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bae5520",
      "metadata": {
        "tags": [],
        "id": "7bae5520"
      },
      "outputs": [],
      "source": [
        "input_documents = []\n",
        "\n",
        "for input_line in input_lines:\n",
        "    if (input_line[\"ActSceneLine\"] != \"\"):\n",
        "        (act, scene, line) = input_line[\"ActSceneLine\"].split(\".\")\n",
        "        location = \"Act {}, Scene {}, Line {}\".format(act, scene, line)\n",
        "        metadata = {\"act\": act, \"scene\": scene, \"line\": line}\n",
        "    else:\n",
        "        location = \"\"\n",
        "        metadata = {}\n",
        "    quote_input = \"{} : {} : {}\".format(location, input_line[\"Player\"], input_line[\"PlayerLine\"])\n",
        "    input_document = Document(page_content=quote_input, metadata=metadata)\n",
        "    input_documents.append(input_document)\n",
        "\n",
        "print(f\"Adding {len(input_documents)} documents ... \", end=\"\")\n",
        "vector_store.add_documents(documents=input_documents, batch_size=50)\n",
        "print(\"Done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d162c2d1-188e-43f0-b1c3-342b80641060",
      "metadata": {
        "id": "d162c2d1-188e-43f0-b1c3-342b80641060"
      },
      "source": [
        "## Answer questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5332b838-6c1f-40f4-a29e-2b2d0250f408",
      "metadata": {
        "tags": [],
        "id": "5332b838-6c1f-40f4-a29e-2b2d0250f408"
      },
      "outputs": [],
      "source": [
        "prompt_template_str = \"\"\"Human: Use the following pieces of context to provide a concise answer to the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Assistant:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(prompt_template_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea67346b-d5ca-433d-858e-5c21397f9de5",
      "metadata": {
        "tags": [],
        "id": "ea67346b-d5ca-433d-858e-5c21397f9de5"
      },
      "source": [
        "We choose to use the following LLM model (see [this page](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html#model-parameters-general) for more info):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee8b2ee0-f9bf-4ada-8fde-7d917d89c6fa",
      "metadata": {
        "tags": [],
        "id": "ee8b2ee0-f9bf-4ada-8fde-7d917d89c6fa"
      },
      "outputs": [],
      "source": [
        "model_id = \"anthropic.claude-v2\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4081fd78-1710-4a42-a942-a14f652c854d",
      "metadata": {
        "id": "4081fd78-1710-4a42-a942-a14f652c854d"
      },
      "source": [
        "Here the question-answering function is set up, implementing the RAG pattern:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e82ef49-ffec-4429-bcc2-ea09f50333cd",
      "metadata": {
        "tags": [],
        "id": "4e82ef49-ffec-4429-bcc2-ea09f50333cd"
      },
      "outputs": [],
      "source": [
        "req_accept = \"application/json\"\n",
        "req_content_type = \"application/json\"\n",
        "\n",
        "# This, created from the vector store, will fetch the top relevant documents given a text query\n",
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "def answer_question(question: str, verbose: bool = False) -> str:\n",
        "    if verbose:\n",
        "        print(f\"\\n[answer_question] Question: {question}\")\n",
        "    # Retrieval of the most relevant stored documents from the vector store:\n",
        "    context_docs = retriever.get_relevant_documents(question)\n",
        "    context = \"\\n\".join(doc.page_content for doc in context_docs)\n",
        "    if verbose:\n",
        "        print(\"\\n[answer_question] Context:\")\n",
        "        print(context)\n",
        "    # Filling the prompt template with the current values\n",
        "    llm_prompt_str = prompt.format(\n",
        "        question=question,\n",
        "        context=context,\n",
        "    )\n",
        "    # Invocation of the Amazon Bedrock LLM for text completion -- ultimately obtaining the answer\n",
        "    llm_body = json.dumps({\"prompt\": llm_prompt_str, \"max_tokens_to_sample\": 500})\n",
        "    llm_response = bedrock_runtime.invoke_model(\n",
        "        body=llm_body,\n",
        "        modelId=model_id,\n",
        "        accept=req_accept,\n",
        "        contentType=req_content_type,\n",
        "    )\n",
        "    llm_response_body = json.loads(llm_response[\"body\"].read())\n",
        "    answer = llm_response_body[\"completion\"].strip()\n",
        "    if verbose:\n",
        "        print(f\"\\n[answer_question] Answer: {answer}\\n\")\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "599d8856-921e-4bf4-8979-ff54b13de6d5",
      "metadata": {
        "tags": [],
        "id": "599d8856-921e-4bf4-8979-ff54b13de6d5"
      },
      "outputs": [],
      "source": [
        "my_answer = answer_question(\"Who dies in the story?\")\n",
        "print(\"=\" * 60)\n",
        "print(my_answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db71cc88-61e3-42e5-802b-4ba4eaa795b4",
      "metadata": {
        "id": "db71cc88-61e3-42e5-802b-4ba4eaa795b4"
      },
      "source": [
        "Let's take a look at the RAG process piece-wise:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fbeaf2d-f589-4d04-8447-4b876375f5b1",
      "metadata": {
        "tags": [],
        "id": "6fbeaf2d-f589-4d04-8447-4b876375f5b1"
      },
      "outputs": [],
      "source": [
        "my_answer = answer_question(\"Who dies in the story?\", verbose=True)\n",
        "print(\"=\" * 60)\n",
        "print(my_answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "715ba03a-66d1-47fe-9a8e-6b2713ddd0f9",
      "metadata": {
        "id": "715ba03a-66d1-47fe-9a8e-6b2713ddd0f9"
      },
      "source": [
        "### Interactive QA session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fe0df9c-b24d-4f35-aee4-8bef2dd1e1a6",
      "metadata": {
        "tags": [],
        "id": "9fe0df9c-b24d-4f35-aee4-8bef2dd1e1a6"
      },
      "outputs": [],
      "source": [
        "user_question = \"\"\n",
        "while True:\n",
        "    user_question = input(\"Enter a question (empty to quit):\").strip()\n",
        "    if user_question:\n",
        "        print(f\"Answer ==> {answer_question(user_question)}\")\n",
        "    else:\n",
        "        print(\"[User, AI exeunt]\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f31c2d97",
      "metadata": {
        "id": "f31c2d97"
      },
      "source": [
        "## Additional resources\n",
        "\n",
        "To learn more about Amazon Bedrock, visit this page: [Introduction to Amazon Bedrock](https://github.com/aws-samples/amazon-bedrock-samples/tree/main/introduction-to-bedrock)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}